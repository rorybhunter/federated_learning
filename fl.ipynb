{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms, utils, datasets\n",
    "from argparse import ArgumentParser\n",
    "from torchvision import transforms as tt\n",
    "from torchvision.models import resnet18, ResNet18_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_std(loader):\n",
    "    mean = 0.0\n",
    "    std = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    \n",
    "    for images, _ in loader:\n",
    "        batch_samples = images.size(0)  # Batch size\n",
    "        images = images\n",
    "        images = images.view(batch_samples, 3, 244,244)\n",
    "        mean += images.mean([0, 2, 3]) * batch_samples\n",
    "        std += images.std([0, 2, 3]) * batch_samples\n",
    "        total_samples += batch_samples\n",
    "\n",
    "    mean /= total_samples\n",
    "    std /= total_samples\n",
    "    \n",
    "    print(\"Mean:\", mean)\n",
    "    print(\"Std:\", std)\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "# general reproducibility\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# gpu training specific\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\"\"\"## Partitioning the Data (IID and non-IID)\"\"\"\n",
    "\n",
    "def get_device():\n",
    "    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "def iid_partition(dataset, clients):\n",
    "    \"\"\"\n",
    "    I.I.D paritioning of data over clients\n",
    "    Shuffle the data\n",
    "    Split it between clients\n",
    "\n",
    "    params:\n",
    "      - dataset (torch.utils.Dataset): Dataset containing the Images\n",
    "      - clients (int): Number of Clients to split the data between\n",
    "\n",
    "    returns:\n",
    "      - Dictionary of image indexes for each client\n",
    "    \"\"\"\n",
    "\n",
    "    num_items_per_client = int(len(dataset) / clients)\n",
    "    client_dict = {}\n",
    "    image_idxs = [i for i in range(len(dataset))]\n",
    "\n",
    "    for i in range(clients):\n",
    "        client_dict[i] = set(np.random.choice(image_idxs, num_items_per_client, replace=False))\n",
    "        image_idxs = list(set(image_idxs) - client_dict[i])\n",
    "\n",
    "    return client_dict\n",
    "\n",
    "\n",
    "def non_iid_partition(dataset, n_nets, alpha):\n",
    "    \"\"\"\n",
    "        :param dataset: dataset name\n",
    "        :param n_nets: number of clients\n",
    "        :param alpha: beta parameter of the Dirichlet distribution\n",
    "        :return: dictionary containing the indexes for each client\n",
    "    \"\"\"\n",
    "    print('non iid setup')\n",
    "    y_train = np.array(dataset.targets)\n",
    "    min_size = 0\n",
    "    K = len(np.unique(y_train))\n",
    "    N = y_train.shape[0]\n",
    "    print(N)\n",
    "    net_dataidx_map = {}\n",
    "\n",
    "    while min_size < 10:\n",
    "        idx_batch = [[] for _ in range(n_nets)]\n",
    "        # for each class in the dataset\n",
    "        for k in range(K):\n",
    "            idx_k = np.where(y_train == k)[0]\n",
    "            np.random.shuffle(idx_k)\n",
    "            proportions = np.random.dirichlet(np.repeat(alpha, n_nets))\n",
    "            ## Balance\n",
    "            proportions = np.array([p * (len(idx_j) < N / n_nets) for p, idx_j in zip(proportions, idx_batch)])\n",
    "            proportions = proportions / proportions.sum()\n",
    "            proportions = (np.cumsum(proportions) * len(idx_k)).astype(int)[:-1]\n",
    "            idx_batch = [idx_j + idx.tolist() for idx_j, idx in zip(idx_batch, np.split(idx_k, proportions))]\n",
    "            min_size = min([len(idx_j) for idx_j in idx_batch])\n",
    "\n",
    "    for j in range(n_nets):\n",
    "        np.random.shuffle(idx_batch[j])\n",
    "        net_dataidx_map[j] = np.array(idx_batch[j])\n",
    "\n",
    "    # net_dataidx_map is a dictionary of length #of clients: {key: int, value: [list of indexes mapping the data among the workers}\n",
    "    # traindata_cls_counts is a dictionary of length #of clients, basically assesses how the different labels are distributed among\n",
    "    # the client, counting the total number of examples per class in each client.\n",
    "    print('partitioj done')\n",
    "    return net_dataidx_map\n",
    "\n",
    "\n",
    "\"\"\"## Federated Averaging\n",
    "\n",
    "### Local Training (Client Update)\n",
    "\n",
    "Local training for the model on client side\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset, idxs):\n",
    "        self.dataset = dataset\n",
    "        self.idxs = list(idxs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image, label = self.dataset[self.idxs[item]]\n",
    "        return image, label\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    images = [img.expand(3, -1, -1) if img.shape[0] == 1 else img for img in images]\n",
    "    return torch.stack(images), torch.tensor(labels)\n",
    "\n",
    "class ClientUpdate(object):\n",
    "    def __init__(self, dataset, batchSize, learning_rate, epochs, idxs, sch_flag):\n",
    "        self.train_loader = DataLoader(CustomDataset(dataset, idxs), batch_size=batchSize, shuffle=True, collate_fn=collate_fn)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.sch_flag = sch_flag\n",
    "\n",
    "    def train(self, model):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        # optimizer = torch.optim.SGD(model.parameters(), lr=self.learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=self.learning_rate)\n",
    "        # if self.sch_flag == True:\n",
    "        #    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5)\n",
    "        # my_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.99)\n",
    "        e_loss = []\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            train_loss = 0.0\n",
    "\n",
    "            model.train()\n",
    "            for data, labels in self.train_loader:\n",
    "                torch.cuda.empty_cache()\n",
    "                if data.size()[0] < 2:\n",
    "                    continue;\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "                if torch.cuda.is_available():\n",
    "                    model.cuda()\n",
    "                    data, labels = data.cuda(), labels.cuda()\n",
    "                # if torch.cuda.is_available():\n",
    "                #     # data, labels = data.cuda(), labels.cuda()\n",
    "                #     torch.cuda.empty_cache()\n",
    "                #     data, labels = data.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "\n",
    "                # clear the gradients\n",
    "                optimizer.zero_grad()\n",
    "                # make a forward pass\n",
    "                output = model(data)\n",
    "                # calculate the loss\n",
    "                loss = criterion(output, labels)\n",
    "                # do a backwards pass\n",
    "                loss.backward()\n",
    "                # perform a single optimization step\n",
    "                optimizer.step()\n",
    "                # update training loss\n",
    "                train_loss += loss.item() * data.size(0)\n",
    "                # if self.sch_flag == True:\n",
    "                #  scheduler.step(train_loss)\n",
    "            # average losses\n",
    "            train_loss = train_loss / len(self.train_loader.dataset)\n",
    "            e_loss.append(train_loss)\n",
    "\n",
    "            # self.learning_rate = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        total_loss = sum(e_loss) / len(e_loss)\n",
    "\n",
    "        return model.state_dict(), total_loss\n",
    "\n",
    "\n",
    "\"\"\"### Server Side Training\n",
    "\n",
    "Following Algorithm 1 from the paper\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def training(model, rounds, batch_size, lr, ds, data_dict, C, K, E, plt_title, plt_color, cifar_data_test,\n",
    "             test_batch_size, criterion, num_classes, classes_test, sch_flag):\n",
    "    \"\"\"\n",
    "    Function implements the Federated Averaging Algorithm from the FedAvg paper.\n",
    "    Specifically, this function is used for the server side training and weight update\n",
    "\n",
    "    Params:\n",
    "      - model:           PyTorch model to train\n",
    "      - rounds:          Number of communication rounds for the client update\n",
    "      - batch_size:      Batch size for client update training\n",
    "      - lr:              Learning rate used for client update training\n",
    "      - ds:              Dataset used for training\n",
    "      - data_dict:       Type of data partition used for training (IID or non-IID)\n",
    "      - C:               Fraction of clients randomly chosen to perform computation on each round\n",
    "      - K:               Total number of clients\n",
    "      - E:               Number of training passes each client makes over its local dataset per round\n",
    "      - tb_writer_name:  Directory name to save the tensorboard logs\n",
    "    Returns:\n",
    "      - model:           Trained model on the server\n",
    "    \"\"\"\n",
    "\n",
    "    # global model weights\n",
    "    global_weights = model.state_dict()\n",
    "\n",
    "    # training loss\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    test_accuracy = []\n",
    "    best_accuracy = 0\n",
    "    # measure time\n",
    "    start = time.time()\n",
    "\n",
    "    for curr_round in range(1, rounds + 1):\n",
    "        w, local_loss = [], []\n",
    "        # Retrieve the number of clients participating in the current training\n",
    "        m = max(int(C * K), 1)\n",
    "        # Sample a subset of K clients according with the value defined before\n",
    "        S_t = np.random.choice(range(K), m, replace=False)\n",
    "        # For the selected clients start a local training\n",
    "        for k in S_t:\n",
    "            torch.cuda.empty_cache()  # Free up unused memory\n",
    "\n",
    "            # Compute a local update\n",
    "            local_update = ClientUpdate(dataset=ds, batchSize=batch_size, learning_rate=lr, epochs=E, idxs=data_dict[k],\n",
    "                                        sch_flag=sch_flag)\n",
    "            # Update means retrieve the values of the network weights\n",
    "            weights, loss = local_update.train(model=copy.deepcopy(model))\n",
    "\n",
    "            w.append(copy.deepcopy(weights))\n",
    "            local_loss.append(copy.deepcopy(loss))\n",
    "        lr = 0.99*lr\n",
    "        # updating the global weights\n",
    "        weights_avg = copy.deepcopy(w[0])\n",
    "        for k in weights_avg.keys():\n",
    "            for i in range(1, len(w)):\n",
    "                weights_avg[k] += w[i][k]\n",
    "\n",
    "            weights_avg[k] = torch.div(weights_avg[k], len(w))\n",
    "\n",
    "        global_weights = weights_avg\n",
    "\n",
    "        # if curr_round == 200:\n",
    "        #     lr = lr / 2\n",
    "        #     E = E - 1\n",
    "\n",
    "        # if curr_round == 300:\n",
    "        #     lr = lr / 2\n",
    "        #     E = E - 2\n",
    "\n",
    "        # if curr_round == 400:\n",
    "        #     lr = lr / 5\n",
    "        #     E = E - 3\n",
    "\n",
    "        # move the updated weights to our model state dict\n",
    "        model.load_state_dict(global_weights)\n",
    "\n",
    "        # loss\n",
    "        loss_avg = sum(local_loss) / len(local_loss)\n",
    "        train_loss.append(loss_avg)\n",
    "\n",
    "        t_accuracy, t_loss = testing(model, cifar_data_test, test_batch_size, criterion, num_classes, classes_test)\n",
    "        test_accuracy.append(t_accuracy)\n",
    "        test_loss.append(t_loss)\n",
    "\n",
    "        if best_accuracy < t_accuracy:\n",
    "            best_accuracy = t_accuracy\n",
    "        # torch.save(model.state_dict(), plt_title)\n",
    "        print(f\"Round {curr_round}, loss_avg: {loss_avg}, t_loss: {t_loss}, test_acc: {test_accuracy[0]}, best_acc: {best_accuracy}\")\n",
    "        # print(curr_round, loss_avg, t_loss, test_accuracy[0], best_accuracy)\n",
    "        # print('best_accuracy:', best_accuracy, '---Round:', curr_round, '---lr', lr, '----localEpocs--', E)\n",
    "\n",
    "    end = time.time()\n",
    "    plt.rcParams.update({'font.size': 8})\n",
    "    fig, ax = plt.subplots()\n",
    "    x_axis = np.arange(1, rounds + 1)\n",
    "    y_axis1 = np.array(train_loss)\n",
    "    y_axis2 = np.array(test_accuracy)\n",
    "    y_axis3 = np.array(test_loss)\n",
    "\n",
    "    ax.plot(x_axis, y_axis1, 'tab:' + 'green', label='train_loss')\n",
    "    ax.plot(x_axis, y_axis2, 'tab:' + 'blue', label='test_accuracy')\n",
    "    ax.plot(x_axis, y_axis3, 'tab:' + 'red', label='test_loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.set(xlabel='Number of Rounds', ylabel='Train Loss',\n",
    "           title=plt_title)\n",
    "    ax.grid()\n",
    "    # fig.savefig(plt_title+'.jpg', format='jpg')\n",
    "    print(\"Training Done!\")\n",
    "    print(\"Total time taken to Train: {}\".format(end - start))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "class MyGroupNorm(nn.Module):\n",
    "    def __init__(self, num_channels):\n",
    "        super(MyGroupNorm, self).__init__()\n",
    "        self.norm = nn.GroupNorm(num_groups=2, num_channels=num_channels,\n",
    "                                 eps=1e-5, affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\"\"\"## Testing Loop\"\"\"\n",
    "\n",
    "\n",
    "def testing(model, dataset, bs, criterion, num_classes, classes):\n",
    "    # test loss\n",
    "    test_loss = 0.0\n",
    "    correct_class = list(0. for i in range(num_classes))\n",
    "    total_class = list(0. for i in range(num_classes))\n",
    "\n",
    "    test_loader = DataLoader(dataset, batch_size=bs, collate_fn=collate_fn)\n",
    "    l = len(test_loader)\n",
    "    model.eval()\n",
    "    print(\"testing\")\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "            if torch.cuda.is_available():\n",
    "                data, labels = data.cuda(), labels.cuda()\n",
    "    \n",
    "            output = model(data)\n",
    "            loss = criterion(output, labels)\n",
    "            test_loss += loss.item() * data.size(0)\n",
    "    \n",
    "            _, pred = torch.max(output, 1)\n",
    "    \n",
    "            correct_tensor = pred.eq(labels.data.view_as(pred))\n",
    "            correct = np.squeeze(correct_tensor.numpy()) if not torch.cuda.is_available() else np.squeeze(\n",
    "                correct_tensor.cpu().numpy())\n",
    "    \n",
    "            # test accuracy for each object class\n",
    "            for i in range(num_classes):\n",
    "                label = labels.data[i]\n",
    "                correct_class[label] += correct[i].item()\n",
    "                total_class[label] += 1\n",
    "\n",
    "    # avg test loss\n",
    "    test_loss = test_loss / len(test_loader.dataset)\n",
    "\n",
    "    return 100. * np.sum(correct_class) / np.sum(total_class), test_loss\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "\n",
    "    # parser = ArgumentParser()\n",
    "    # parser.add_argument('--norm', default=\"bn\")\n",
    "    # parser.add_argument('--partition', default=\"noniid\")\n",
    "    # parser.add_argument('--client_number', default=100)\n",
    "    # parser.add_argument('--alpha_partition', default=0.5)\n",
    "    # parser.add_argument('--commrounds', type=int, default=100)\n",
    "    # parser.add_argument('--clientfr', type=float, default=0.1)\n",
    "    # parser.add_argument('--numclient', type=int, default=100)\n",
    "    # parser.add_argument('--clientepochs', type=int, default=20)\n",
    "    # parser.add_argument('--clientbs', type=int, default=64)\n",
    "    # parser.add_argument('--clientlr', type=float, default=0.001)\n",
    "    # parser.add_argument('--sch_flag', default=False)\n",
    "\n",
    "    # args = parser.parse_args()\n",
    "    \n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.norm = \"bn\"\n",
    "        self.partition = \"noniid\"\n",
    "        self.client_number = 20\n",
    "        self.alpha_partition = 1\n",
    "        self.commrounds = 30\n",
    "        self.clientfr = 0.25\n",
    "        self.numclient = 20\n",
    "        self.clientepochs = 10\n",
    "        self.clientbs = 16\n",
    "        self.clientlr = 0.001\n",
    "        self.sch_flag = False\n",
    "\n",
    "args = Args()\n",
    "\n",
    "\n",
    "# classes = np.array(list(cifar_data_train.class_to_idx.values()))\n",
    "# classes_test = np.array(list(cifar_data_test.class_to_idx.values()))\n",
    "# num_classes = len(classes_test)\n",
    "\n",
    "\n",
    "# custom dataset class for knee x_ray dataset\n",
    "import os\n",
    "from torchvision.io import read_image\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, annotations_file, img_dir, transform, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform((image.float()/255.0))\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        # return image.permute(1,2,0), label\n",
    "        return image, label\n",
    "\n",
    "    @property\n",
    "    def targets(self):\n",
    "        return list(self.img_labels.values[:, 1])\n",
    "\n",
    "\n",
    "# Compute mean and std\n",
    "# mean, std = compute_mean_std(loader)\n",
    "normalisation_stats = ((0.2654), (0.2872)) # ( (mean), (std) )\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((265,265)),\n",
    "    # transforms.CenterCrop((512,240)),\n",
    "    transforms.CenterCrop((512,244)),\n",
    "    transforms.Normalize(*normalisation_stats),\n",
    "    transforms.Grayscale(num_output_channels=3)\n",
    "])\n",
    "\n",
    "train_annotation_file_path = \"/kaggle/input/knee-xray-split-dataset/knee_xray_split_dataset/train/train_annotations.csv\"\n",
    "test_annotation_file_path = \"/kaggle/input/knee-xray-split-dataset/knee_xray_split_dataset/test/test_annotations.csv\"\n",
    "train_img_file_path = \"/kaggle/input/knee-xray-split-dataset/knee_xray_split_dataset/train\"\n",
    "test_img_file_path = \"/kaggle/input/knee-xray-split-dataset/knee_xray_split_dataset/test\"\n",
    "\n",
    "\n",
    "'''\n",
    "This is the mixed dataset that had test and train images mixed, so results are invalid\n",
    "\n",
    "# train_annotation_file_path = \"/kaggle/input/large-knee-xray-data/large-knee-xray/train/annotations.csv\"\n",
    "# test_annotation_file_path = \"/kaggle/input/large-knee-xray-data/large-knee-xray/train/annotations.csv\"\n",
    "# train_img_file_path = \"/kaggle/input/large-knee-xray-data/large-knee-xray/train\"\n",
    "# test_img_file_path = \"/kaggle/input/large-knee-xray-data/large-knee-xray/train\"\n",
    "'''\n",
    "\n",
    "xray_train = CustomImageDataset(train_annotation_file_path,\n",
    "                                      train_img_file_path,\n",
    "                                      transform=transform)\n",
    "xray_test = CustomImageDataset(test_annotation_file_path,\n",
    "                                      test_img_file_path,\n",
    "                                      transform=transform)\n",
    "\n",
    "# xray_train = dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "classes = np.array([0,1])\n",
    "classes_test = np.array([0,1])\n",
    "num_classes = len(classes_test)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Hyperparameters_List (H) = [rounds, client_fraction, number_of_clients, number_of_training_rounds_local, local_batch_size, lr_client]\n",
    "H = [args.commrounds, args.clientfr, args.numclient, args.clientepochs, args.clientbs, args.clientlr]\n",
    "\n",
    "if args.partition == 'noniid':\n",
    "    # (dataset, clients, total_shards, shards_size, num_shards_per_client):\n",
    "    # alpha for the Dirichlet distribution\n",
    "    print('creating noniid partition')\n",
    "    data_dict = non_iid_partition(xray_train, args.client_number, float(args.alpha_partition))\n",
    "else:\n",
    "    print('creating iid partition')\n",
    "    data_dict = iid_partition(xray_train, 100)  # Uncomment for idd_partition\n",
    "\n",
    "# if args.norm == 'gn':\n",
    "#     cifar_cnn = resnet.ResNet(resnet.Bottleneck, [3, 4, 6, 3], num_classes=10, zero_init_residual=False, groups=1,\n",
    "#                               width_per_group=64, replace_stride_with_dilation=None, norm_layer=MyGroupNorm)\n",
    "# else:\n",
    "#     cifar_cnn = resnet.ResNet(resnet.Bottleneck, [3, 4, 6, 3], num_classes=10, zero_init_residual=False, groups=1,\n",
    "#                               width_per_group=64, replace_stride_with_dilation=None)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "# model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3,bias=False)\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, 2)\n",
    "# model = CustomVGG19(num_classes=2)\n",
    "\n",
    "print('model created')\n",
    "\n",
    "device=get_device()\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "to_device(model, device)\n",
    "\n",
    "plot_str = args.partition + '_' + args.norm + '_' + 'comm_rounds_' + str(args.commrounds) + '_clientfr_' + str(\n",
    "    args.clientfr) + '_numclients_' + str(args.numclient) + '_clientepochs_' + str(\n",
    "    args.clientepochs) + '_clientbs_' + str(args.clientbs) + '_clientLR_' + str(args.clientlr)\n",
    "print(plot_str)\n",
    "\n",
    "trained_model = training(model, H[0], H[4], H[5], xray_train, data_dict, H[1], H[2], H[3], plot_str,\n",
    "                         \"green\", xray_test, 128, criterion, num_classes, classes_test, args.sch_flag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming `model` is your trained model\n",
    "torch.save(trained_model, 'model.pth')\n",
    "\n",
    "# To load it back:\n",
    "# model = 'model.pth'\n",
    "# model = '/kaggle/input/96pc/pytorch/default/1/96pc.pth'\n",
    "model = trained_model\n",
    "# model = torch.load(model, weights_only=False)\n",
    "model.eval()  # Set to evaluation mode if needed\n",
    "\n",
    "# Assuming you have a test dataset\n",
    "test_loader = DataLoader(xray_test, batch_size=32, shuffle=False, collate_fn=collate_fn)  # Adjust batch size as needed\n",
    "correct = 0\n",
    "total = 0\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():  # No need to track gradients during evaluation\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        _, predicted = torch.max(outputs, 1)  # Get class with highest probability\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
